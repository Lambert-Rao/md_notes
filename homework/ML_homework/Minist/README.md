
### 概述
在本次实验中，我探索了使用反向传播（Backpropagation, BP）神经网络对MNIST手写数字识别任务进行性能优化的策略。MNIST数据集是一个广泛应用的手写数字图片集，含有60000个训练样本和10000个测试样本。每个样本都是一张28x28像素的灰度图像，代表从0至9的数字。

[toc]

### 算法
反向传播（Backpropagation）是一种应用于多层人工神经网络的梯度下降算法。该算法使用链式法则，以网络每层的权重为变量计算损失函数的梯度，以此更新权重，从而最小化损失函数。

反向传播算法（BP算法）主要由两个阶段组成：激励传播与权重更新。

**第一阶段：激励传播**

每次迭代中的传播环节包含两步：

- （前向传播阶段）将训练输入送入网络以获得预测结果。
- （反向传播阶段）对预测结果与训练目标进行差值运算（即损失函数的计算）。

**第二阶段：权重更新**

对于每个突触上的权重，按照以下步骤进行更新：

- 将输入激励与回应误差相乘，从而计算权重的梯度。
- 将此梯度乘以一个比例因子并取反，然后加到权重上。

这个比例因子将会影响训练过程的速度和效果，因此将其称为"训练因子"。由于梯度的方向指向误差增大的方向，在更新权重时需将其取反，以减小权重引起的误差。

这两个阶段可以反复进行，直到网络对输入的响应达到预定的目标范围。

#### BP算法

1. **权重初始化**：这是神经网络训练的第一步。一般会选择较小的随机值作为初始权重，这种做法有助于神经网络更有效地从数据中学习。

2. **前向传播**：在神经网络的每一层中，输入值会乘以权重并加上一个偏置，然后通过一个激活函数，如Sigmoid函数，ReLU函数等进行非线性变换。具体到每一个神经元，假设其输入为$x$，权重为$w$，偏置为$b$，那么该神经元的加权和可以表示为：

$$
z = w \cdot x + b
$$

   然后，$z$被送入激活函数$f$，得到该神经元的输出值：

$$
   y = f(z)
$$

3. **误差计算**：通过比较神经网络的预测输出和实际目标值来计算误差。误差通常使用损失函数来计算，例如，最常用的均方误差损失函数可以表示为：
$$
   E = \frac{1}{2} \sum (target - output)^2
$$
4. **反向传播误差**：这是反向传播算法的关键步骤。需要计算损失函数相对于每个权重的梯度。这一步使用了链式法则，根据损失函数的导数和每一层输出的导数，逐层向后计算误差的梯度。误差梯度 $\delta$ 可以表示为：
$$
\delta = (target - output) \cdot f'(z)
$$
   其中，$f'(z)$ 是激活函数的导数。

5. **权重更新**：根据计算出的误差梯度，进行权重的更新。权重的更新遵循梯度下降法则，即向误差减小的方向调整权重。新的权重是通过旧的权重减去学习率乘以误差梯度得到的：
$$
w_{new} = w_{old} - \text{learning\_rate} \cdot \delta
$$

以上步骤会在每个训练样本上重复执行，直到网络的预测误差满足预设的阈值，或者达到预设的迭代次数为止。在每个训练周期内，前向传播和反向传播过程都会执行一次。

###  实验过程
在本次实验中，设计并实现了一个具有单一隐藏层的神经网络。该网络包括一个输入层，一个隐藏层和一个输出层。输入层包含了784个节点，每个节点对应一个28x28像素的图像中的一个像素。这样的设计可以有效地处理图像数据，并充分利用输入信息。隐藏层含有30个节点，足够大的规模使其具有较高的模型复杂度，可以捕获许多复杂的特征和模式。输出层有10个节点，代表了10个分类结果，即0至9的数字。选用了Sigmoid函数作为神经元的激活函数，其典型的S形曲线可以有效地平滑输入值，使得小的改变在输出端产生大的效果，有助于神经网络的学习和优化。

为了最大限度地提高模型的性能，采用了两种优化策略：多线程训练和动态学习率调整。首先，进行了基于单线程的直接训练作为基准，然后，为了提高训练效率，引入了多线程训练方法，这种方法将数据集分成多个部分并同时进行训练，可以显著地减少训练时间，使得神经网络可以更快地收敛。最后，采用了动态学习率调整策略，这种策略将学习率在训练过程中逐渐减小，这样做可以提高模型的准确性并避免过拟合。逐渐减小的学习率也能帮助模型在训练早期快速接近最优解，然后在训练后期更精细地进行调整。

> 可以看到我们的CPU利用率大大提高了。

![image-20230622083435255](https://raw.githubusercontent.com/Limpol-Rao/image_host/main/img/202306220834869.png)

#### **训练结果**

```
epoch = 1,  average precision = 0.925898, average recall = 0.925072, average F1 score = 0.924915, time:2
epoch = 10, average precision = 0.959579, average recall = 0.959338, average F1 score = 0.959403, time:19
epoch = 20, average precision = 0.961497, average recall = 0.961233, average F1 score = 0.96132 , time:38
epoch = 30, average precision = 0.962786, average recall = 0.962541, average F1 score = 0.962621, time:57
epoch = 40, average precision = 0.963542, average recall = 0.963396, average F1 score = 0.963427, time:76
epoch = 50, average precision = 0.963211, average recall = 0.9631  , average F1 score = 0.963115, time:96
epoch = 60, average precision = 0.963025, average recall = 0.9629  , average F1 score = 0.962922, time:116
```

#### **实验结果综述与分析**

**动态学习率**

- **精度**：随着训练的进行，模型的精度逐步提升，直到第12轮达到了约0.96的水平。在此之后，精度的增长变得非常缓慢，几乎保持在0.963的水平。这表明在12轮后，模型可能已经达到了它的学习极限，或者说已经收敛。此种情况可能是因为随着训练的深入，模型已经从数据中学习到了大量有用的信息，进一步的训练带来的信息增益较小。
- **时间性能**：从每个epoch所需的时间来看，观察到几乎呈现线性增长的趋势，从最初的epoch仅需2秒，到最后一个epoch需耗时116秒，平均每个epoch大约需2秒。这个时间性能表现在合理范围内，因为模型在每一个epoch中处理的数据量都是一样的。这一点显著优于单线程训练，后者每个epoch平均需要12秒的时间。

**多线程固定学习率**

- **精度**：训练初期，精度迅速上升，然后在第7轮时达到峰值，约为0.9577。然而，随后的训练过程中，精度开始出现轻微下滑，并且呈现一定的波动，总体上并未观察到显著的提升。
- **时间性能**：对于每个epoch所需的时间，也显示出线性增长的趋势，从最初的epoch仅需2秒，到最后一个epoch需耗时274秒，平均每个epoch也大约需2秒。然而，在相同epoch下，采用多线程固定学习率的策略所得到的模型精度相对较低。

**综合分析**

在动态学习率和多线程固定学习率两种训练策略中，动态学习率在整体的精度上表现更优。它能在较少的epoch中达到较高的精度，并且在后续的训练过程中，精度的变动非常小，表现出了很好的稳定性。相比之下，虽然多线程固定学习率在时间性能上与动态学习率相当，但其模型精度较低，在相同的epoch下，多线程固定学习率所训练的模型精度略低于动态学习率模型。

#### 实验结论

本次实验设计了一个单一隐藏层的神经网络，通过对其进行深入研究和多次实验，我得到了一些重要的结论。首先，网络的设计在图像识别的场景中具有很好的适应性和准确性，输入层的节点数量恰好对应28x28像素的图像，使得模型能够充分利用图像信息进行有效学习。

在进行模型优化的过程中，我采用了多线程训练和动态学习率调整两种策略。实验结果表明，多线程训练有效地提升了模型的训练效率，大幅度减少了训练时间，而动态学习率调整策略则进一步提升了模型的准确性并有效避免了过拟合，两者配合使用使得模型达到了最佳的学习效果。

从实验结果来看，我的神经网络模型在经过适当的训练和优化后，表现出了较高的准确性，证明了我们的设计和实施策略是有效的。

然而，值得注意的是，虽然我们的模型在本实验中表现出了良好的性能，但这并不能保证它在所有情况下都能有同样的效果。因此，我们在将该模型应用到其他场景或问题之前，应该先进行适当的调整和优化，以确保其能够适应不同的数据和任务需求。

### 参考

[Principles of training multi-layer neural network using backpropagation](https://wiki.eecs.yorku.ca/course_archive/2011-12/F/4403/_media/backpropagation.pdf)

[BP实现示例]()

周志华《机器学习》

李航《统计学习方法》

### 附录

> 代码全文如下

 <iframe src="https://raw.githubusercontent.com/Limpol-Rao/image_host/main/img/202306221247250.cpp" > </iframe>